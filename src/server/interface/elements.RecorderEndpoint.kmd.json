{
  "remoteClasses": [
    {
      "name": "RecorderEndpoint",
      "extends": "UriEndpoint",
      "doc": "Provides functionality to store media contents.
<p>
  RecorderEndpoint can store media into local files or send it to a remote
  network storage. When another :rom:cls:`MediaElement` is connected to a
  RecorderEndpoint, the media coming from the former will be encapsulated into
  the selected recording format and stored in the designated location.
</p>
<p>
  These parameters must be provided to create a RecorderEndpoint, and they
  cannot be changed afterwards:
</p>
<ul>
  <li>
    <strong>Destination URI</strong>, where media will be stored. These formats
    are supported:
    <ul>
      <li>
        File: A file path that will be written into the local file system.
        Example:
        <ul>
          <li><code>file:///path/to/file</code></li>
        </ul>
      </li>
      <li>
        HTTP: A POST request will be used against a remote server. The server
        must support using the <i>chunked</i> encoding mode (HTTP header
        <code>Transfer-Encoding: chunked</code>). Examples:
        <ul>
          <li><code>http(s)://{server-ip}/path/to/file</code></li>
          <li>
            <code>
              http(s)://{username}:{password}@{server-ip}:{server-port}/path/to/file
            </code>
          </li>
        </ul>
      </li>
      <li>
        Relative URIs (with no schema) are supported. They are completed by
        prepending a default URI defined by property <i>defaultPath</i>. This
        property is defined in the configuration file
        <i>/etc/kurento/modules/kurento/UriEndpoint.conf.ini</i>, and the
        default value is <code>file:///var/lib/kurento/</code>
      </li>
      <li>
        <strong>
          NOTE (for current versions of Kurento 6.x): special characters are not
          supported in <code>{username}</code> or <code>{password}</code>.
        </strong>
        This means that <code>{username}</code> cannot contain colons
        (<code>:</code>), and <code>{password}</code> cannot contain 'at' signs
        (<code>@</code>). This is a limitation of GStreamer 1.8 (the underlying
        media framework behind Kurento), and is already fixed in newer versions
        (which the upcoming Kurento 7.x will use).
      </li>
      <li>
        <strong>
          NOTE (for upcoming Kurento 7.x): special characters in
          <code>{username}</code> or <code>{password}</code> must be
          url-encoded.
        </strong>
        This means that colons (<code>:</code>) should be replaced with
        <code>%3A</code>, and 'at' signs (<code>@</code>) should be replaced
        with <code>%40</code>.
      </li>
    </ul>
  </li>
  <li>
    <strong>Media Profile</strong> (:rom:attr:`MediaProfileSpecType`), used for
    storage. This will determine the video and audio encoding. See below for
    more details about Media Profile.
  </li>
  <li>
    <strong>EndOfStream</strong> (optional), a parameter that dictates if the
    recording should be automatically stopped once the EOS event is detected.
  </li>
</ul>
<p>
  Note that
  <strong>
    RecorderEndpoint requires write permissions to the destination
  </strong>
  ; otherwise, the media server won't be able to store any information, and an
  :rom:evt:`Error` will be fired. Make sure your application subscribes to this
  event, otherwise troubleshooting issues will be difficult.
</p>
<ul>
  <li>
    To write local files (if you use <code>file://</code>), the system user that
    is owner of the media server process needs to have write permissions for the
    requested path. By default, this user is named '<code>kurento</code>'.
  </li>
  <li>
    To record through HTTP, the remote server must be accessible through the
    network, and also have the correct write permissions for the destination
    path.
  </li>
</ul>
<p>
  The <strong>Media Profile</strong> is quite an important parameter, as it will
  determine whether the server needs to perform on-the-fly transcoding of the
  media. If the input stream codec if not compatible with the selected media
  profile, the media will be transcoded into a suitable format. This will result
  in a higher CPU load and will impact overall performance of the media server.
</p>
<p>
  For example: If your pipeline receives <b>VP8</b>-encoded video from WebRTC,
  and sends it to a RecorderEndpoint; depending on the format selected...
</p>
<ul>
  <li>
    WEBM: The input codec is the same as the recording format, so no transcoding
    will take place.
  </li>
  <li>
    MP4: The media server will have to transcode from <b>VP8</b> to <b>H264</b>.
    This will raise the CPU load in the system.
  </li>
  <li>
    MKV: Again, video must be transcoded from <b>VP8</b> to <b>H264</b>, which
    means more CPU load.
  </li>
</ul>
<p>
  From this you can see how selecting the correct format for your application is
  a very important decision.
</p>
<p>
  Recording will start as soon as the user invokes the
  <code>record</code> method. The recorder will then store, in the location
  indicated, the media that the source is sending to the endpoint. If no media
  is being received, or no endpoint has been connected, then the destination
  will be empty. The recorder starts storing information into the file as soon
  as it gets it.
</p>
<p>
  <strong>Recording must be stopped</strong> when no more data should be stored.
  This can be with the <code>stopAndWait</code> method, which blocks and returns
  only after all the information was stored correctly.
</p>
<p>
  <strong>
    If your output file is empty, this means that the recorder is waiting for
    input media.
  </strong>
  When another endpoint is connected to the recorder, by default both AUDIO and
  VIDEO media types are expected, unless specified otherwise when invoking the
  <code>connect</code> method. Failing to provide both types, will result in the
  RecorderEndpoint buffering the received media: it won't be written to the file
  until the recording is stopped. The recorder waits until all types of media
  start arriving, in order to synchronize them appropriately.
</p>
<p>
  The source endpoint can be hot-swapped while the recording is taking place.
  The recorded file will then contain different feeds. When switching video
  sources, if the new video has different size, the recorder will retain the
  size of the previous source. If the source is disconnected, the last frame
  recorded will be shown for the duration of the disconnection, or until the
  recording is stopped.
</p>
<p>
  <strong>
    It is recommended to start recording only after media arrives.
  </strong>
  For this, you may use the <code>MediaFlowInStateChange</code> and
  <code>MediaFlowOutStateChange</code>
  events of your endpoints, and synchronize the recording with the moment media
  comes into the Recorder. For example:
</p>
<ol>
  <li>
    When the remote video arrives to KMS, your WebRtcEndpoint will start
    generating packets into the Kurento Pipeline, and it will trigger a
    <code>MediaFlowOutStateChange</code> event.
  </li>
  <li>
    When video packets arrive from the WebRtcEndpoint to the RecorderEndpoint,
    the RecorderEndpoint will raise a <code>MediaFlowInStateChange</code> event.
  </li>
  <li>
    You should only start recording when RecorderEndpoint has notified a
    <code>MediaFlowInStateChange</code> for ALL streams (so, if you record
    AUDIO+VIDEO, your application must receive a
    <code>MediaFlowInStateChange</code> event for audio, and another
    <code>MediaFlowInStateChange</code> event for video).
  </li>
</ol>
      ",
      "constructor":
        {
          "doc": "Builder for the :rom:cls:`RecorderEndpoint`",
          "params": [
            {
              "name": "mediaPipeline",
              "doc": "the :rom:cls:`MediaPipeline` to which the endpoint belongs",
              "type": "MediaPipeline"
            },
            {
              "name": "uri",
              "doc": "URI where the recording will be stored. It must be accessible from the media server process itself:
              <ul>
                <li>Local server resources: The user running the Kurento Media Server must have write permission over the file.</li>
                <li>Network resources: Must be accessible from the network where the media server is running.</li>
              </ul>",
              "type": "String"
            },
            {
              "name": "mediaProfile",
              "doc": "Sets the media profile used for recording. If the profile is different than the one being received at the sink pad, media will be transcoded, resulting in a higher CPU load. For instance, when recording a VP8 encoded video from a WebRTC endpoint in MP4, the load is higher that when recording to WEBM.",
              "type": "MediaProfileSpecType",
              "optional": true,
              "defaultValue": "WEBM"
            },
            {
              "name": "stopOnEndOfStream",
              "doc": "Forces the recorder endpoint to finish processing data when an :term:`EOS` is detected in the stream",
              "type": "boolean",
              "optional": true,
              "defaultValue": false
            }
          ]
        },
      "methods": [
        {
          "name": "record",
          "doc": "Starts storing media received through the sink pad.",
          "params": []
        },
        {
          "name": "stopAndWait",
          "doc": "Stops recording and does not return until all the content has been written to the selected uri. This can cause timeouts on some clients if there is too much content to write, or the transport is slow",
          "params": []
        }
      ],
      "events": [
        "Recording",
        "Paused",
        "Stopped"
      ]
    }
  ],
  "events": [
    {
      "name": "Recording",
      "extends": "Media",
      "doc": "Fired when the recoding effectively starts. ie: Media is received by the recorder and record method has been called.",
      "properties": []
    },
    {
      "name": "Paused",
      "extends": "Media",
      "doc": "@deprecated</br>Fired when the recorder goes to pause state",
      "properties": []
    },
    {
      "name": "Stopped",
      "extends": "Media",
      "doc": "@deprecated</br>Fired when the recorder has been stopped and all the media has been written to storage.",
      "properties": []
    }
  ],
  "complexTypes": [
    {
      "name": "GapsFixMethod",
      "typeFormat": "ENUM",
      "doc": "How to fix gaps when they are found in the recorded stream.
<p>
  Gaps are typically caused by packet loss in the input streams, such as when an
  RTP or WebRTC media flow suffers from network congestion and some packets
  don't arrive at the media server.
</p>
<p>Different ways of handling gaps have different tradeoffs:</p>
<ul>
  <li>
    <strong>NONE</strong>: Do not fix gaps. This means that the
    resulting files will contain gaps in the timestamps. Some players are clever
    enough to adapt to this during playback, so that the gaps are reduced to a
    minimum and no problems are perceived by the user; other players are not so
    sophisticated, and will struggle trying to decode a file that contains gaps.
    For example, trying to play such a file directly with Chrome will cause
    lipsync issues (audio and video out of sync).
    <p>
      For example, assume a session length of 15 seconds: packets arrive
      correctly during the first 5 seconds, then there is a gap, then data
      arrives again for the last 5 seconds. Also, for simplicity, assume 1 frame
      per second. With no fix for gaps, the RecorderEndpoint will store each
      frame as-is, with these timestamps:
    </p>
    <pre>
      frame 1  - 00:01
      frame 2  - 00:02
      frame 3  - 00:03
      frame 4  - 00:04
      frame 5  - 00:05
      frame 11 - 00:11
      frame 12 - 00:12
      frame 13 - 00:13
      frame 14 - 00:14
      frame 15 - 00:15
    </pre>
    <p>
      Notice how the frames between 6 to 10 are missing, but the last 5 frames
      still conserve their original timestamp. The total length of the file is
      detected as 15 seconds by most players, although playback could stutter or
      hang during the missing section.
    </p>
  </li>
  <li>
    <strong>GENPTS</strong>: Replace timestamps of all frames arriving
    after a gap. With this method, the length of each gap will be taken into
    account, to be subtracted from the timestamp of all following frames. This
    provides for a continuous, gap-less recording, at the expense of reducing
    the total apparent length of each file.
    <p>
      In our example, the RecorderEndpoint will change all timestamps that
      follow a gap in the stream, and store each frame as follows:
    </p>
    <pre>
      frame 1  - 00:01
      frame 2  - 00:02
      frame 3  - 00:03
      frame 4  - 00:04
      frame 5  - 00:05
      frame 11 - 00:06
      frame 12 - 00:07
      frame 13 - 00:08
      frame 14 - 00:09
      frame 15 - 00:10
    </pre>
    <p>
      Notice how the frames between 6 to 10 are missing, and the last 5 frames
      have their timestamps corrected to provide a smooth increment over the
      previous ones. The total length of the file is detected as 10 seconds, and
      playback should be correct throughout the whole file.
    </p>
  </li>
  <li>
    <strong>FILL_IF_TRANSCODING</strong>: NOT IMPLEMENTED.
    <p>This is a proposal for future improvement of the RecorderEndpoint.</p>
    <p>
      It is possible to perform a dynamic adaptation of audio rate and add frame
      duplication to the video, such that the missing parts are filled with
      artificial data. This has the advantage of providing a smooth playback
      result, and at the same time conserving all original timestamps.
    </p>
    <p>
      However, the main issue with this method is that it requires accessing the
      decoded media; i.e., transcoding must be active. For this reason, the
      proposal is to offer this option to be enabled only when transcoding would
      still happen anyways.
    </p>
    <p>
      In our example, the RecorderEndpoint would change all missing frames like
      this:
    </p>
    <pre>
      frame 1  - 00:01
      frame 2  - 00:02
      frame 3  - 00:03
      frame 4  - 00:04
      frame 5  - 00:05
      fake frame - 00:06
      fake frame - 00:07
      fake frame - 00:08
      fake frame - 00:09
      fake frame - 00:10
      frame 11 - 00:11
      frame 12 - 00:12
      frame 13 - 00:13
      frame 14 - 00:14
      frame 15 - 00:15
    </pre>
    <p>
      This joins the best of both worlds: on one hand, the playback should be
      smooth and even the most basic players should be able to handle the
      recording files without issue. On the other, the total length of the file
      is left unmodified, so it matches with the expected duration of the
      sessions that are being recorded.
    </p>
  </li>
</ul>
      ",
      "values": [
        "NONE",
        "GENPTS",
        "FILL_IF_TRANSCODING"
      ]
    }
  ]
}
